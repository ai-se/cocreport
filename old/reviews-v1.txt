       >>> Discussion Summary <<<

This is a very well written paper very carefully crafted however there are
major problems. The major concerns are as follows:

the evaluation was made on the same dataset used to define the model and tailor
model parameters;

the model has a high number of parameters and a posteriori it can be used to
fit almost anything

 a more realistic evaluation would imply 1) use COCOMO on a new dataset NOT
used to define the model and not used to tailor parameters moreover WITHOUT
knowing the effort thus estimate the parameter with no bias. 2) After having
made the prediction compare estimates with recorded effort.

To simplify to the bare bone this paper concludes that a two-decade old
universal model (COCOMO II) does not need to be tuned and performs better than
serious competing alternatives based on data analysis. It also support the
claim that COCOMO II is not sensitive to large errors in system size.

This is quite troubling as if a large error on the input causes only small
variation of the output either there is magic involved or the parameters are
set to reduce the input variability.

*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=

First reviewer's review:

          >>> Summary of the submission <<<

The paper challenges the common assumption that old parametric models have been
superseded by more modern effort estimation approaches. It compares COCOMO (and
derived) parametric models against pure LOC, Knear and CART estimates.  Findings
support the claim that parametric models (at least the COCOMO family) are still
useful and that (at least for the Knear and CART) there is no real benefit in
using more modern estimation approaches.

          >>> Evaluation <<<

Pros:

not often investigated topic

sound methodology and evaluation approach

quite clear and strongly supported findings

Cons:

challenging COCOMO with just blind LOC estimates may not be really meaningful
unless you
do not use specific company or project based back-firing coefficients. Even in
this
situation one should account for the different kind of LOC aka 100% reused,
partially
reused with minor changes or generated. The LOC of a GUI are not equal to the
LOC of a
language parser and a language parser manually written is not the same of a
parser
generated by Yacc.

the real comparison is COCOMO vs Knear and CART; these are just two possibility
out of
many the claimed superiority of Knear and CART is based on published data but
are they
really in the field the best one can do?




This is a paper easy to read and understand (at least the main message). From a
philosophical standpoint I find not surprising that an equation with 5+17
parameters can
accommodate almost any possible situation. Sure there is a comparison between
COCOMO base
and COCOME II but still the number of parameter combination and factors is
simply
staggering. A developer by selecting factors and parameters can obtain quite
different
estimates and at that point she have to decide what to believe. Luckily she can
claim she
used an industrial strength approach.

I am also wondering what are the implication of using a leave-one-out approach.
If my set
of project is say 17 (Nasa 10) I will learn on 16 and predict one. How realistic
of a
company is to have a backlog of 16 homogeneous projects? In the paper it is not
very
clear how parameters for the new project have been assigned. I assume Fig 1 code
is to
guess the best estimates of model parameters given the training but then you
have to
decide the specific value for the test. Did the evaluation assume all parameters
constant
but the size? I mean, assuming training fixed the 5 multipliers, did the
evaluation
consider all 17 factors with different and new weights? The project is a new
one, people
may have change, and so on. Please clarify.



Minor comments
==============


Line 11 of abstract remove the duplicated these

Conclusion: RQ3 remove the extra weird char

*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*

Second reviewer's review:

          >>> Summary of the submission <<<

This paper revisits important questions regarding the use of parametric effort
estimation models, with a specific focus on COCOMO. For that purpose,
experiments are performed on 4 distinct COMO data sets, including the original
and more recent ones. First, it investigates whether attributes contribute to
better estimates than just using size (LOCs) alone. Results show that indeed
attributes contribute to better estimates. Second, the paper compares COCOMO
with two well known estimation methods: CART (regression trees) and a
nearest-neighbor approach (Case-based reasoning). Results show that COCOMO
performed overall better. Third, the paper investigates whether COCOMO local
calibration can be improved by the COCONUT algorithm. Results show that it is
not better than untuned COCOMO in terms of MRE but that it decreases variance.
Fourth, the paper explores various ways to simplify the use and deployment of
COCOMO parametric models. Results it is defensible to learn a COCOMO model from
eight projects only and that a three-point scale for attributes is enough.
Fifth, the sensitivity of estimates to size estimates (LOCS) is investigated.
Though MRE tends to increase by the introduction of errors in size estimates,
the model is not unduly sensitive up to 50% of their original value.

          >>> Evaluation <<<

The paper is overall well written and the experimental procedures that are
followed are well described and correctly used.

However, there are issues to be pointed out, the first one being the most
significant:

Data sets:

I must confess I was a bit surprised by the results, that large errors
introduced in system size would have little effect on effort prediction, that
decade-old tunings would work better than local tuning, etc. And I could not
help but think that this was related to the data sets selected, which are all
COMOMO datasets involved in improving or evaluating the COCOMO model.
What I have observed in practice with people trying to apply COCOMO to past
projects, in an a posteriori fashion, is that they adjust the attribute values
to fit the model, to obtain better predictions ... This is easily done since
such attributes, or at least most of them, are inherently subjective and must be
interpreted in context. So, in the end, what you obtain are data sets that are
working very well for COCOMO. This might very well explain why COCOMO yields
better results overall than alternatives.
Such adjustments of attribute values is not malicious, but people face
inaccurate predictions, try to understand why, and tend to attribute them to
their misunderstanding of the model attributes and their interpretations.
Of course, in a normal situation, when setting these values in early project
stages, one sets these attributes based on subjective
interpretation of what they mean in a specific context, and one cannot assess
whether the estimate is accurate and if the attribute values need to be
adjusted.
My conclusion is that I think your results are strongly biased since they
are based on COCOMO datasets following the typical bias I have mentioned above,
i.e., a posteriori adjustments of attribute values. If you really wanted to
answer your questions, with a high degree of credibility, you should have people
estimate attributes in the early stages of development, perform predictions when
the actual effort is now yet known and no attribute adjustment is possible,
using all estimation methods, and then compare their results.


Motivations:

The paper claims that parametric estimation models are widely used and
widely useful. However, the paper does not properly discuss that the
majority of software projects are  estimated based on human judgment
(expert estimation), and that (at least to my knowledge), available
research does not suggest that expert estimation should be replaced by
parametric models. Therefore, statements like "is still valid and
recommended practice to first try parametric estimation" would benefit
from being somewhat moderated. It would be interesting if the brief
mentioning of combination of estimation methods (e.g. regarding when
and how) could be elaborated somewhat.

I think that the relevance of research question 1 should be documented
better. Is this really a common criticism - the only places I have seen
such simple models (similar to LOC) applied in practice, is for
estimation in very similar contexts (typically same organization, same
implementation team, same technology, similar size, etc).


System size (LOCs):

The results about the sensitivity of models to size (LOCs) is very surprising.
How can an augmentation or reduction of 50% in size have little effect when all
other factors are kept constant? What does that say about the model?
In practice, we do not know— or at least it is not discussed in the paper
—how accurate are size estimates. A related question is why do we expect size
estimates to be easier than direct effort estimates? Are they? To me that would
be a more important question that the sensitivity of models to system size. It
would not be wrong for a model to be, after all, sensitive to how large the
system to be developed is.
The fact that COCOMO is not as sensitive as expected is rather strange. But the
procedure followed for this analysis is not clear: How is the variation of
25%/50% performed exactly? in which direction?

Cost of deployment:

Regarding RQ4 ("is parametric estimation expensive to deploy at some
new site?"),  I think the research question should be rephrased to
better describe what has actually been investigated, i.e. is it
possible to (meaningfully) reduce the cost of deploying parametric
estimation compared to current practice.  To answer the original
question, another study design would be needed (e.g. factors like
training, data collection should be discussed across models).

*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*

Third reviewer's review:

          >>> Summary of the submission <<<

The paper describes an interesting observation regarding the use of parametric
effort estimation models to support decision making in software projects. By
discussing the history of parametric models and the rationale behind such
models, a set of datasets are used to support the observation of the feasibility
of keeping use these models to estimate software project effort.

          >>> Evaluation <<<

Favour Points

Well written and explicit paper
Empirical strategy sounds and completely explained
Explanations support study replication (except the reuse of 2 datasets: COC5 and
NASA10)



Against Points
I missed some discussion regarding the equivalence of projects data into the
COC5 and NASA10 datasets (same company, multi companies?).
Data validity is an issue
Missed an explanation regarding set the confidence level to 99%. why not 95%


General comments

I really like to read this paper. It is clear, explicit and offers historical
information that is important to set the ground of the software technology under
observation. The empirical strategy you have used to observe the behavior of
parametric models is also good. All the criteria and decision making are based
on evidence, what makes your findings and observations strong.

This paper is a good piece of work to be available to the SE community. Besides
the acquired evidence and empirical approach, it offers a synthesized discussion
about estimation models and their use. The reading by students, novice
researchers and practitioners will be naturally motivated by the type of
information and facility of understanding.

Some minor issues were identified in the text, that would like to share with
you:

Typos and Grammatical issues

Intro, first bullet: ... make almost exclusive use OF parametric...
II, A: DELPHI-vs...,
IV, C, 4) Results:
... In Figure 9, all r4...treatments. That is, THESE results...
... Overall, Figure 9...Hence, it needS not be...

Equations and Figures:

All the equations did not print in right way! Terms collapsed, overlap or just
got into the regular text. Please, double check it.

Graph on Figure 4 needs to be enlarged (hard to visualize)
Figures 7, 8, 9 and 10 all the effect sizes are marked with S and not with the
expected symbol

*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*